{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99119b09-5a8f-49f5-98da-56b488440690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification,pipeline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import xmltodict\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "529808e0-6259-4c89-a3e4-16c550884c57",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CellText Mining\n",
    "\n",
    "\n",
    "### Functions contained in this notebook are for extracting raw text from Cell journal articles and then running NER on the output.\n",
    "    The actual text json itself is acquired from Elsevier's TDM api. You will need to request access, and use your own API key (when in development) to run the following code.\n",
    "    Light cleaning is done on the output to sanitize it and remove artifacts from stemming and the tokenization process, however, the validity of the output is not guaranteed and required further validation.\n",
    "    The models used for NER come from huggingface and are all based on biobert or pubmedbert, and specialized for NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d2cf4-8bf1-4572-9675-89139e4923c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV of articles, must have column for DOIs of articles\n",
    "input_df = pd.read_csv('/NLP/CellArticles.csv')\n",
    "## If necessary, filter out those that have Open Access (don't have to, can be used for validation of Open acess TDM methods)\n",
    "input_df = input_df[input_df[\"Open Access\"].isna()]\n",
    "## Import NER models from HuggingFace. Feel free to import different NER models if you wish\n",
    "disease_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "disease_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "genetic_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "genetic_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "pubmedbert_gene = AutoTokenizer.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Gene\", model_max_length=512)\n",
    "pubmedbert_gene_model = AutoModelForTokenClassification.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Gene\")\n",
    "pubmedbert_disease = AutoTokenizer.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Disease\",model_max_length=512)\n",
    "pubmedbert_disease_model = AutoModelForTokenClassification.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Disease\")\n",
    "\n",
    "disease_nlp = pipeline(\"ner\", model=disease_model, tokenizer=disease_tokenizer)\n",
    "genetic_nlp = pipeline(\"ner\", model=genetic_model, tokenizer=genetic_tokenizer)\n",
    "pubmedbert_gene_nlp = pipeline(\"ner\", model=pubmedbert_gene_model, tokenizer=pubmedbert_gene)\n",
    "pubmedbert_disease_nlp = pipeline(\"ner\", model=pubmedbert_disease_model, tokenizer=pubmedbert_disease)\n",
    "\n",
    "## Whatever models you choose, seperate them out into gene and disease, and collect them as two seperate arrays\n",
    "\n",
    "disease = [disease_nlp,pubmedbert_disease_nlp]\n",
    "genetic = [genetic_nlp,pubmedbert_gene_nlp]\n",
    "\n",
    "\n",
    "nlps={'disease':disease,'genetic':genetic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35e5fa-49f6-4d44-b8dc-d25dff8a9f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Use Cell API and token to query article\n",
    "def getCellArticles(cell_doi):\n",
    "    cell_api_key = '1f89a8d2a51cc28137532f5f47bbb032'\n",
    "    cell_token= 'da050463085352e8a83c00a3fe1e7aac'\n",
    "    cell_query = f'https://api.elsevier.com/content/article/doi/{cell_doi}?APIKey={cell_api_key}&insttoken={cell_token}&view=FULL'\n",
    "    response = requests.get(cell_query)\n",
    "    dict_data = xmltodict.parse(response.content)\n",
    "    decoded_response = response.content.decode(\"utf-8\")\n",
    "    response_json = json.loads(json.dumps(xmltodict.parse(decoded_response)))\n",
    "    # Turn query into json and flatten into a string\n",
    "    cell_article = (list(flatten(getKey(response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item'],'#text'))))\n",
    "    return cell_article\n",
    "\n",
    "## Some basic data processing\n",
    "def df_cleaning(df):\n",
    "    new_entries = []\n",
    "    ends = []\n",
    "    starts = []\n",
    "    entities = []\n",
    "    for i, row in df.iterrows():\n",
    "        # if the entry starts with '##', combine it with the previous entry\n",
    "        if row['word'].startswith('##'):\n",
    "            try:\n",
    "                new_entries[-1] = new_entries[-1].strip() + row['word'][2:].strip() ##Strip out '##' unless unable to, in which case skip and continue\n",
    "            except:\n",
    "                continue\n",
    "            ends[-1] = row['end']\n",
    "        else:\n",
    "            new_entries.append(row['word'].strip()) ##If no '##' to strip, append as is to array of results\n",
    "            ends.append(row['end'])\n",
    "            starts.append(row['start'])\n",
    "            entities.append(row['entity'])\n",
    "    concatenated_text = []\n",
    "    if not new_entries:\n",
    "        return(pd.DataFrame())\n",
    "    current_text = new_entries[0]\n",
    "    current_start = starts[0]\n",
    "    current_end = ends[0]\n",
    "    current_ent = entities[0]\n",
    "    for i in range(1, len(new_entries)): ##Scans through all results in array, concatenating consectutive terms to form longer string results\n",
    "        if starts[i] == current_end+1 or starts[i] == current_end:\n",
    "            current_text = current_text + \" \" +new_entries[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "        else:\n",
    "            concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "            current_text = new_entries[i]\n",
    "            current_start = starts[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "    concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "    return pd.DataFrame(concatenated_text, columns=['entity','word', 'start', 'end'])\n",
    "\n",
    "## Recursively extracts info from a dict. In this case we are extracting all '#text' fields\n",
    "def getKey(d, key): \n",
    "    res_list = []\n",
    "    fin_list = []\n",
    "    try:\n",
    "        res_list.append(d[key])\n",
    "    except:\n",
    "        try:\n",
    "            for i in d.keys():\n",
    "                res_list.append(getKey(d[i], key))\n",
    "        except:\n",
    "            if type(d) is list:\n",
    "                for i in d:\n",
    "                    if type(i) is dict:\n",
    "                        res_list.append(getKey(i, key))\n",
    "    for i in res_list:\n",
    "        if i:\n",
    "            fin_list.append(i)\n",
    "    return(fin_list)\n",
    "\n",
    "## Unwind json\n",
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i \n",
    "\n",
    "## Run NLP on string input and returns results as DF\n",
    "def NER_results(nlps,string_result):\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in nlps:\n",
    "        temp_df = pd.DataFrame(i(string_result))\n",
    "        temp_df = df_cleaning(temp_df)\n",
    "        try:\n",
    "            temp_df = temp_df[temp_df[\"entity\"] != '0']\n",
    "        except:\n",
    "            pass\n",
    "        result_df = pd.concat([result_df, temp_df])\n",
    "    result_df.sort_values(by=['start'])\n",
    "    return result_df\n",
    "\n",
    "## Cleanup any collisons/overlaps\n",
    "def collision_cleanup(model_type,full_output):\n",
    "    output= pd.DataFrame()\n",
    "    full_output[full_output.columns[0]] = model_type\n",
    "    process_df = full_output.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "    df = process_df.drop_duplicates() ##Handle simple true duplicates\n",
    "    while output.equals(df) == False:\n",
    "        output = df\n",
    "        df = df.sort_values(['start','end'])\n",
    "        c1 = df['word'].shift() == df['word']\n",
    "        c2 = df['end'].shift() - df['start'] <= 0\n",
    "        #c3 = df['end'].shift() - df['end'] < 0\n",
    "        df['interval'] = df['end'] - df['start']\n",
    "        df['overlap'] = (c1 | c2).cumsum()\n",
    "        df = df.sort_values(['interval'], ascending=False).groupby('overlap').first()\n",
    "        df = df.reset_index(drop=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddd8c6-8d7c-4339-b24f-d507a208f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = {}\n",
    "fail = []\n",
    "## Run NER over all articles, if failed, store doi in array. Successful results are stored in a dictionary with the DOI as the key\n",
    "for doi in input_df['DOI']:\n",
    "    result = pd.DataFrame()\n",
    "    print(f'\\r{doi}', end='',)\n",
    "    try:\n",
    "        string_result = ''.join(getCellArticles(doi))\n",
    "        for i in nlps:\n",
    "            temp=collision_cleanup(i,(NER_results(nlps[i],string_result)))\n",
    "            result = pd.concat([result, temp])\n",
    "        result = result.sort_values(by=['start']).reset_index(drop=True)\n",
    "        success[doi] = [string_result,result]\n",
    "    except:\n",
    "        fail.append([doi,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cf45f-89aa-451a-b03f-246a154e5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Some simple clean up around em dashes.\n",
    "for i in success:\n",
    "    replace = []\n",
    "    for j in success[i][1]['word']:\n",
    "        replace.append(j.replace(\" - \", \"-\"))\n",
    "    success[i][1]['word'] = replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed42e22-c657-4388-a752-826df6904e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate classifications of histone modifiers. This part is optional and can be swapped out at will for any other classification of a gene product\n",
    "reader = pd.read_csv('/NLP/readertbl.csv')\n",
    "writer = pd.read_csv('/NLP/writertbl.csv')\n",
    "eraser = pd.read_csv('/NLP/erasertbl.csv')\n",
    "reader['classification']= 'reader'\n",
    "writer['classification']= 'writer'\n",
    "eraser['classification']= 'eraser'\n",
    "classification_df = pd.concat([reader, writer,eraser]).reset_index(drop=True)\n",
    "classification_df['gene'] = classification_df['gene'].str.lower()\n",
    "\n",
    "##Scan through DF output and match with a knowledge base\n",
    "for i in success:\n",
    "    success[i][1]['classification'] = 'NULL'\n",
    "    queries = (set(list(success[i][1]['word'])) & set(classification_df['gene'])) ##Speed up look up by creating sets \n",
    "    for query in queries:\n",
    "        index = classification_df[classification_df['gene']==query].index.values\n",
    "        classification = list(classification_df.loc[index, 'classification'])\n",
    "        index_change = list(success[i][1][success[i][1]['word']==query].index.values)\n",
    "        for j in index_change:\n",
    "            success[i][1].loc[[j], 'classification'] = pd.Series([classification], index=success[i][1].index[[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf741ae-b8c8-4c21-bf7d-4c862b4e929b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('savedCellArticles.pkl', 'wb') as f:\n",
    "#     pickle.dump(success, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72448c3e-39cd-4ade-8456-223d927751b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('savedCellArticles.pkl', 'rb') as f:\n",
    "    success = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef1942-21aa-4b9f-85d6-facb00617c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9505718-7de7-42eb-ab9a-c553feb568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['head'] \n",
    "##Bolded \"abstract/intro\" text and data availability statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01b4f4-384d-42ea-9482-23cdabdb1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['tail'] \n",
    "##Citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74003d92-5b67-427e-904f-3f7485f0574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['body']['ce:appendices'] \n",
    "##No supplemental text itself, may not be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb398d4-6536-40ce-b03c-d521eca9e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['body']['ce:sections']['ce:section'][1]['ce:section'][0]\n",
    "#Not all sections structured the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375c687b-a9c6-44a5-9f55-2f661829cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for i in success:\n",
    "    for j in success[i][1]['classification']:\n",
    "        if isinstance(j, list):\n",
    "            article_list.append(i)\n",
    "result = [*set(article_list)]\n",
    "sample = random.sample(result, 10)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c8e7a4-47f8-4bdb-9725-cd977def7849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.1016/j.bbagrm.2018.10.019',\n",
       " '10.1016/j.beha.2004.08.011',\n",
       " '10.1016/j.jbior.2012.04.003',\n",
       " '10.1016/j.dnarep.2009.04.003',\n",
       " '10.1016/j.tig.2006.09.007',\n",
       " '10.1016/j.mce.2017.03.016',\n",
       " '10.1016/j.dnarep.2011.01.012',\n",
       " '10.1016/j.jmb.2008.09.011',\n",
       " '10.1016/j.currproblcancer.2018.03.001',\n",
       " '10.1016/j.ejmg.2019.103739']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b3cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
