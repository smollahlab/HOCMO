{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e214f17f-7ade-441d-a0b9-ddf6353e278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification,pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import xmltodict\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab859555-c3eb-40f1-a929-a995b30aba09",
   "metadata": {},
   "source": [
    "# OpenAccess Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab262f-029c-4120-8130-0bfe146d02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = './.cache'\n",
    "\n",
    "\n",
    "def read_text_files_to_df(folder_path, encoding='UTF-8'):\n",
    "    file_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_list.append(os.path.join(folder_path, file))\n",
    "    \n",
    "    data = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            with open(file, 'r', encoding=encoding) as f:\n",
    "                data.append(f.read())\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "                data.append(f.read())\n",
    "    \n",
    "    df = pd.DataFrame({'content': data})\n",
    "    return df\n",
    "\n",
    "def check_keywords(row):\n",
    "    for keyword in keywords:\n",
    "        if keyword not in row['content']:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def NER_per_text(df, nlp, i, threshold):\n",
    "    start = 0 \n",
    "    end = 512\n",
    "\n",
    "    sentence = df['content'][i][start:end]\n",
    "    result = nlp(sentence)\n",
    "    final = pd.DataFrame(result)\n",
    "\n",
    "    while end < len(df['content'][i]):\n",
    "        start = end +1\n",
    "        end = start + 512\n",
    "        if end > len(df['content'][i]):\n",
    "            end = len(df['content'][i])\n",
    "        sentence = df['content'][i][start:end]\n",
    "        result = pd.DataFrame(nlp(sentence))\n",
    "        if not result.empty:\n",
    "            result['start'] +=  start\n",
    "            result['end'] += start\n",
    "        final = final.append(result, ignore_index=True)\n",
    "    final = final[final['score'] >= threshold]\n",
    "    final = final.sort_values(by=['start'])\n",
    "    return final\n",
    "\n",
    "def df_cleaning(df):\n",
    "    new_entries = []\n",
    "    ends = []\n",
    "    starts = []\n",
    "    entities = []\n",
    "    for i, row in df.iterrows():\n",
    "        # if the entry starts with '##', combine it with the previous entry\n",
    "        if row['word'].startswith('##'):\n",
    "            try:\n",
    "                new_entries[-1] = new_entries[-1].strip() + row['word'][2:].strip()\n",
    "            except:\n",
    "                continue\n",
    "            ends[-1] = row['end']\n",
    "        else:\n",
    "            new_entries.append(row['word'].strip())\n",
    "            ends.append(row['end'])\n",
    "            starts.append(row['start'])\n",
    "            entities.append(row['entity'])\n",
    "    concatenated_text = []\n",
    "    if not new_entries:\n",
    "        return(pd.DataFrame())\n",
    "    current_text = new_entries[0]\n",
    "    current_start = starts[0]\n",
    "    current_end = ends[0]\n",
    "    current_ent = entities[0]\n",
    "    for i in range(1, len(new_entries)):\n",
    "        if starts[i] == current_end+1 or starts[i] == current_end:\n",
    "            current_text = current_text + \" \" +new_entries[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "        else:\n",
    "            concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "            current_text = new_entries[i]\n",
    "            current_start = starts[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "    concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "    return pd.DataFrame(concatenated_text, columns=['entity','word', 'start', 'end'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d42632-c0ba-4736-bb67-17fddb002dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32789569-0264-4fe7-8999-dfce4293a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Corpus Aggregation\n",
    "df1 = read_text_files_to_df(\"PMC001xxxxxx\")\n",
    "df2 = read_text_files_to_df(\"PMC002xxxxxx\")\n",
    "df3 = read_text_files_to_df(\"PMC003xxxxxx\")\n",
    "df4 = read_text_files_to_df(\"PMC004xxxxxx\")\n",
    "df5 = read_text_files_to_df(\"PMC005xxxxxx\")\n",
    "df6 = read_text_files_to_df(\"PMC006xxxxxx\")\n",
    "df7 = read_text_files_to_df(\"PMC007xxxxxx\")\n",
    "df8 = read_text_files_to_df(\"PMC008xxxxxx\")\n",
    "df9 = read_text_files_to_df(\"PMC009xxxxxx\")\n",
    "#df = pd.concat([df1, df2, df3, df4, df5, ])\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9])\n",
    "keywords = ['cancer', 'histone modifier']\n",
    "df = df[df.apply(check_keywords, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00041bf-409e-48ae-b4c2-81759d90674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict()\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"drAbreu/bioBERT-NER-BC2GM_corpus\")\n",
    "disease_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "disease_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "genetic_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "genetic_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "disease_nlp = pipeline(\"ner\", model=disease_model, tokenizer=disease_tokenizer)\n",
    "genetic_nlp = pipeline(\"ner\", model=genetic_model, tokenizer=genetic_tokenizer)\n",
    "\n",
    "for i in df.index:\n",
    "    temp_df = NER_per_text(df,disease_nlp,i,0.85)\n",
    "    temp_df = temp_df[temp_df['entity'] != '0']\n",
    "    temp_df = df_cleaning(temp_df)\n",
    "    temp_df = temp_df.append(df_cleaning(NER_per_text(df,genetic_nlp,i,0.85)))\n",
    "    temp_df.sort_values(by=['start'])\n",
    "    result_dict[i] = temp_df\n",
    "ent = []\n",
    "for i in result_dict.items():\n",
    "    for j in (i[1]['word']):\n",
    "        ent.append(j)\n",
    "ent = set(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c6c8a-aa30-4b2c-ae66-fa5fa4d178af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd97ed-fa55-4359-b45a-7665a46c02bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db6651-5dc2-40bc-9e06-2930bd7fd0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e9ca2-8d98-4a27-9c7b-5cf6e6c046ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f54e32-9cb8-415e-93b9-5c306bbb1681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc11442-00a2-45ae-9baa-0bdddac6d01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9f746-0e14-4e0e-81ee-1a84b6f9e5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd15c8-44ce-4bbe-832d-ecdb2b662397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8582dfd8-50da-4d47-9fb0-99e236e5d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacc482-93cb-4cf4-9732-062ca9538d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06bec48-9bf1-4e62-aebe-c66be9f89744",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 588 Full articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1a55a-cd30-4f5b-8022-d5f9ed4bc984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf741ae-b8c8-4c21-bf7d-4c862b4e929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da85e5-9a2f-4d80-8bdb-b9363a2c3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec265b9b-073a-4771-8478-11a56dc22400",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json= get_json('10.1016/j.cell.2022.05.010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391afb6-fb53-4311-863a-b2fd86c10e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['simple-article'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f2524-6801-41e9-8d42-c4f20cb1cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'article' in response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf3c0e-8f55-4155-90b6-bc13240be804",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b7380-8464-4ff6-9495-0c6d8c6a2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['simple-article']['body']['ce:sections']['ce:section'][0]['ce:section'][0]['ce:para'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823801d-0929-4adb-a696-7691c0bdcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b063b8-f6dc-4afc-bd04-9f521f3dbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafcf60-d107-47b8-9c17-f8b891c362b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658b093-3e6c-4c88-8ff9-30566f7bcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json['full-text-retrieval-response']['coredata']['dc:description'] ##Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27614c70-03c2-4832-9482-6c2439854776",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json['full-text-retrieval-response']['coredata']['dcterms:subject'] ##Relevant topics -- Screen here for histones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9505718-7de7-42eb-ab9a-c553feb568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['head'] \n",
    "##Bolded \"abstract/intro\" text and data availability statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01b4f4-384d-42ea-9482-23cdabdb1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['tail'] \n",
    "##Citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74003d92-5b67-427e-904f-3f7485f0574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['body']['ce:appendices'] \n",
    "##No supplemental text itself, may not be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb398d4-6536-40ce-b03c-d521eca9e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_json['full-text-retrieval-response']['originalText']['xocs:doc']['xocs:serial-item']['article']['body']['ce:sections']['ce:section'][1]['ce:section'][0]\n",
    "#Not all sections structured the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962380d-e5bf-491a-8083-4865a906039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_results(nlps,string_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57453477-bd3e-4d38-a12e-ef1f82cb61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_df[input_df[\"Open Access\"].isna()]\n",
    "disease_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "disease_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "genetic_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "genetic_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "disease_nlp = pipeline(\"ner\", model=disease_model, tokenizer=disease_tokenizer)\n",
    "genetic_nlp = pipeline(\"ner\", model=genetic_model, tokenizer=genetic_tokenizer)\n",
    "nlps=[disease_nlp,genetic_nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "eb15e157-8e2f-40b6-8c5b-3eaf7d71bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i \n",
    "\n",
    "def getKey(d, key):\n",
    "    res_list = []\n",
    "    fin_list = []\n",
    "    try:\n",
    "        res_list.append(d[key])\n",
    "    except:\n",
    "        try:\n",
    "            for i in d.keys():\n",
    "                res_list.append(getKey(d[i], key))\n",
    "        except:\n",
    "            if type(d) is list:\n",
    "                for i in d:\n",
    "                    if type(i) is dict:\n",
    "                        res_list.append(getKey(i, key))\n",
    "    for i in res_list:\n",
    "        if i:\n",
    "            fin_list.append(i)\n",
    "    return(fin_list)\n",
    "\n",
    "def getNatureArticles(doi):\n",
    "    api_key = '499d84c073f0ade469f211fd37104d7d'\n",
    "    query = f'https://spdi.public.springernature.app/xmldata/jats?q=doi:{doi}&api_key={api_key}/wustl-api'\n",
    "    response = requests.get(query)\n",
    "    dict_data = xmltodict.parse(response.content)\n",
    "    nature_article = (list(flatten(getKey(dict_data['response']['records']['article']['body'],'#text'))))\n",
    "    return(nature_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73edec0-d547-46b7-9ec1-b8fd8beddb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = []\n",
    "fail = []\n",
    "for i in input_df['DOI']:\n",
    "    print(f'\\r{i}', end='',)\n",
    "    try:\n",
    "        string_result = ''.join(getNatureArticles(doi))\n",
    "        success.append(NER_results(nlps,string_result))\n",
    "    except:\n",
    "        fail.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b75ffb-56ad-4dd4-bc7a-b44a0f4d3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(getNatureArticles(doi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "375c687b-a9c6-44a5-9f55-2f661829cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['@article-type'] ##Could be useful for screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4afd0-fb29-405b-aa3e-d16fdb572204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['front'] ## Author/institutions etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f61c39-b7f3-4256-9c7d-23e9883229f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['back'] ## supplementary/acknowledgements/citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009405c5-7f75-4f07-b829-ad324ff29c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
