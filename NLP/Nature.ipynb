{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e214f17f-7ade-441d-a0b9-ddf6353e278d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification,pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import xmltodict\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pickle \n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f05c637c-fc28-4af8-aada-289f3f708895",
   "metadata": {},
   "source": [
    "# Nature Text Mining\n",
    "\n",
    "## Documentation same as Cell journals. See that notebook for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57453477-bd3e-4d38-a12e-ef1f82cb61ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_df = pd.read_csv('NatureArticles.csv')\n",
    "input_df = input_df[input_df[\"Open Access\"].isna()]\n",
    "disease_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "disease_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\n",
    "genetic_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "genetic_model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "pubmedbert_gene = AutoTokenizer.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Gene\", model_max_length=512)\n",
    "pubmedbert_gene_model = AutoModelForTokenClassification.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Gene\")\n",
    "pubmedbert_disease = AutoTokenizer.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Disease\",model_max_length=512)\n",
    "pubmedbert_disease_model = AutoModelForTokenClassification.from_pretrained(\"pruas/BENT-PubMedBERT-NER-Disease\")\n",
    "\n",
    "disease_nlp = pipeline(\"ner\", model=disease_model, tokenizer=disease_tokenizer)\n",
    "genetic_nlp = pipeline(\"ner\", model=genetic_model, tokenizer=genetic_tokenizer)\n",
    "pubmedbert_gene_nlp = pipeline(\"ner\", model=pubmedbert_gene_model, tokenizer=pubmedbert_gene)\n",
    "pubmedbert_disease_nlp = pipeline(\"ner\", model=pubmedbert_disease_model, tokenizer=pubmedbert_disease)\n",
    "\n",
    "disease = [disease_nlp,pubmedbert_disease_nlp]\n",
    "genetic = [genetic_nlp,pubmedbert_gene_nlp]\n",
    "\n",
    "\n",
    "nlps={'disease':disease,'genetic':genetic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd7e91-903c-42e1-9974-25102500b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i \n",
    "def df_cleaning(df):\n",
    "    new_entries = []\n",
    "    ends = []\n",
    "    starts = []\n",
    "    entities = []\n",
    "    for i, row in df.iterrows():\n",
    "        # if the entry starts with '##', combine it with the previous entry\n",
    "        if row['word'].startswith('##'):\n",
    "            try:\n",
    "                new_entries[-1] = new_entries[-1].strip() + row['word'][2:].strip()\n",
    "            except:\n",
    "                continue\n",
    "            ends[-1] = row['end']\n",
    "        else:\n",
    "            new_entries.append(row['word'].strip())\n",
    "            ends.append(row['end'])\n",
    "            starts.append(row['start'])\n",
    "            entities.append(row['entity'])\n",
    "    concatenated_text = []\n",
    "    if not new_entries:\n",
    "        return(pd.DataFrame())\n",
    "    current_text = new_entries[0]\n",
    "    current_start = starts[0]\n",
    "    current_end = ends[0]\n",
    "    current_ent = entities[0]\n",
    "    for i in range(1, len(new_entries)):\n",
    "        if starts[i] == current_end+1 or starts[i] == current_end:\n",
    "            current_text = current_text + \" \" +new_entries[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "        else:\n",
    "            concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "            current_text = new_entries[i]\n",
    "            current_start = starts[i]\n",
    "            current_end = ends[i]\n",
    "            current_ent = entities[i]\n",
    "    concatenated_text.append((current_ent, current_text, current_start, current_end))\n",
    "    return pd.DataFrame(concatenated_text, columns=['entity','word', 'start', 'end'])\n",
    "\n",
    "\n",
    "def getKey(d, key):\n",
    "    res_list = []\n",
    "    fin_list = []\n",
    "    try:\n",
    "        res_list.append(d[key])\n",
    "    except:\n",
    "        try:\n",
    "            for i in d.keys():\n",
    "                res_list.append(getKey(d[i], key))\n",
    "        except:\n",
    "            if type(d) is list:\n",
    "                for i in d:\n",
    "                    if type(i) is dict:\n",
    "                        res_list.append(getKey(i, key))\n",
    "    for i in res_list:\n",
    "        if i:\n",
    "            fin_list.append(i)\n",
    "    return(fin_list)\n",
    "\n",
    "def NER_results(nlps,string_result):\n",
    "    result_df=pd.DataFrame()\n",
    "    for i in nlps:\n",
    "        temp_df = pd.DataFrame(i(string_result))\n",
    "        temp_df = df_cleaning(temp_df)\n",
    "        try:\n",
    "            temp_df = temp_df[temp_df[\"entity\"] != '0']\n",
    "        except:\n",
    "            pass\n",
    "        result_df = pd.concat([result_df, temp_df])\n",
    "    result_df.sort_values(by=['start'])\n",
    "    return result_df\n",
    "\n",
    "def getNatureArticles(doi):\n",
    "    api_key = '499d84c073f0ade469f211fd37104d7d'\n",
    "    query = f'https://spdi.public.springernature.app/xmldata/jats?q=doi:{doi}&api_key={api_key}/wustl-api'\n",
    "    response = requests.get(query)\n",
    "    dict_data = xmltodict.parse(response.content)\n",
    "    nature_article = (list(flatten(getKey(dict_data['response']['records']['article']['body'],'#text')))) ##Only want books\n",
    "    return(nature_article)\n",
    "\n",
    "def collision_cleanup(model_type,full_output):\n",
    "    output= pd.DataFrame()\n",
    "    full_output[full_output.columns[0]] = model_type\n",
    "    process_df = full_output.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "    df = process_df.drop_duplicates() ##Handle simple true duplicates\n",
    "    while output.equals(df) == False:\n",
    "        output = df\n",
    "        df = df.sort_values(['start','end'])\n",
    "        c1 = df['word'].shift() == df['word']\n",
    "        c2 = df['end'].shift() - df['start'] <= 0\n",
    "        #c3 = df['end'].shift() - df['end'] < 0\n",
    "        df['interval'] = df['end'] - df['start']\n",
    "        df['overlap'] = (c1 | c2).cumsum()\n",
    "        df = df.sort_values(['interval'], ascending=False).groupby('overlap').first()\n",
    "        df = df.reset_index(drop=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73edec0-d547-46b7-9ec1-b8fd8beddb21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "success = {}\n",
    "fail = []\n",
    "for doi in input_df['DOI']:\n",
    "    result = pd.DataFrame()\n",
    "    print(f'\\r{doi}', end='',)\n",
    "    try:\n",
    "        string_result = ''.join(getNatureArticles(doi))\n",
    "        for i in nlps:\n",
    "            temp=collision_cleanup(i,(NER_results(nlps[i],string_result)))\n",
    "            result = pd.concat([result, temp])\n",
    "        result = result.sort_values(by=['start']).reset_index(drop=True)\n",
    "        success[doi] = [string_result,result]\n",
    "    except:\n",
    "        fail.append([doi,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78ae79-165d-4c6f-a3ce-5066b33ec1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in success:\n",
    "    replace = []\n",
    "    for j in success[i][1]['word']:\n",
    "        replace.append(j.replace(\" - \", \"-\"))\n",
    "    success[i][1]['word'] = replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93454a87-9da0-43fd-9f99-177c1efdec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv('readertbl.csv')\n",
    "writer = pd.read_csv('writertbl.csv')\n",
    "eraser = pd.read_csv('erasertbl.csv')\n",
    "reader['classification']= 'reader'\n",
    "writer['classification']= 'writer'\n",
    "eraser['classification']= 'eraser'\n",
    "classification_df = pd.concat([reader, writer,eraser]).reset_index(drop=True)\n",
    "classification_df['gene'] = classification_df['gene'].str.lower()\n",
    "\n",
    "for i in success:\n",
    "    success[i][1]['classification'] = 'NULL'\n",
    "    queries = (set(list(success[i][1]['word'])) & set(classification_df['gene']))\n",
    "    for query in queries:\n",
    "        index = classification_df[classification_df['gene']==query].index.values\n",
    "        classification = list(classification_df.loc[index, 'classification'])\n",
    "        index_change = list(success[i][1][success[i][1]['word']==query].index.values)\n",
    "        for j in index_change:\n",
    "            success[i][1].loc[[j], 'classification'] = pd.Series([classification], index=success[i][1].index[[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c687b-a9c6-44a5-9f55-2f661829cff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['@article-type'] ##Could be useful for screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4afd0-fb29-405b-aa3e-d16fdb572204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['front'] ## Author/institutions etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f61c39-b7f3-4256-9c7d-23e9883229f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dict_data['response']['records']['article']['back'] ## supplementary/acknowledgements/citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0c4ab-ecff-47fd-b64b-7c579ecf6c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('savedNatureArticles.pkl', 'wb') as f:\n",
    "#     pickle.dump(success, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adb8fa1-78d1-4c4b-8969-a541d860f264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('savedNatureArticles.pkl', 'rb') as f:\n",
    "    success = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a59c7-4a48-4d29-80c8-98b3819625b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cb8c18d-628f-4f85-a728-4505787c39b4",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0f11bd-4aae-4c21-8943-6a5a6374e288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62b892d-2446-4fb3-9081-e95abbf98437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedCellArticles.pkl', 'rb') as f:\n",
    "    success1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483ec7ab-cc73-40d5-8647-610a6dd8b8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b243448-e8a5-48d2-b5b7-177e88f062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for i in success:\n",
    "    for j in success[i][1]['classification']:\n",
    "        if isinstance(j, list):\n",
    "            article_list.append(i)\n",
    "result = [*set(article_list)]\n",
    "sample = random.sample(result, 5)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "757f90e2-652c-4e5a-af35-ee4cb0eebe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['10.1016/j.bbagrm.2018.10.019',\n",
    " '10.1016/j.beha.2004.08.011',\n",
    " '10.1016/j.jbior.2012.04.003',\n",
    " '10.1016/j.dnarep.2009.04.003',\n",
    " '10.1016/j.tig.2006.09.007',\n",
    " '10.1016/j.mce.2017.03.016',\n",
    " '10.1016/j.dnarep.2011.01.012',\n",
    " '10.1016/j.jmb.2008.09.011',\n",
    " '10.1016/j.currproblcancer.2018.03.001',\n",
    " '10.1016/j.ejmg.2019.103739',\n",
    " '10.1007/s11010-010-0586-3',\n",
    " '10.1038/s41418-022-00992-3',\n",
    " '10.1038/ni1046',\n",
    " '10.1038/s41388-019-1081-2',\n",
    " '10.1007/s00412-004-0311-7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab231bde-9839-44cd-b45e-71c9c52e51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = success1 | success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44423d4d-cd60-444c-8f24-3af5e9cc3ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd084ddb-2014-42af-9ae1-26a6c1f1bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_subset = dict((k, success[k]) for k in sample if k in merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3f2e71-733b-4810-9f64-7e1c61add2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotation_subset.pkl', 'wb') as f:\n",
    "    pickle.dump(annotation_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194f896-c8c4-4162-a8ec-ec8872f8f7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
