{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80abd173",
   "metadata": {},
   "source": [
    "# Disease analysis\n",
    "\n",
    "This notebook contains code that was used for analyzing specifically disease entities from the NER output (see \"NLP-selected/savedCellArticles.pkl\" and 'NLP-selected/savedNatureArticles.pkl\" for output) in order to categorize them and find the most frequent disease categories. This was part of the process in order to find most frequent re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e6b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aee946",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"..\" # directory containing NER output pickle files and crewDB csv tables\n",
    "\n",
    "########## PREPROCESSING ##########\n",
    "\n",
    "# load data from pickle files\n",
    "# discard the \"text\" data and mark which journal each article originated from\n",
    "with open(f'{dir}/savedCellArticles.pkl', 'rb') as f:\n",
    "    cell_data = pickle.load(f) # 924 articles\n",
    "    filtered_cell_data = {article: [entities, 'cell'] for article, [text, entities] in cell_data.items()}\n",
    "\n",
    "with open(f'{dir}/savedNatureArticles.pkl', 'rb') as f:\n",
    "    nature_data = pickle.load(f) # 401 articles\n",
    "    filtered_nature_data = {article: [entities, 'nature'] for article, [text, entities] in nature_data.items()}\n",
    "\n",
    "# raw combined data of the form {article: [entities, origin]}\n",
    "raw_combined_data = filtered_cell_data | filtered_nature_data # 1325 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110f2743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## UMLS LEXICON NORMALIZATION ##########\n",
    "# for more information about Norm tool see here: https://lhncbc.nlm.nih.gov/LSG/Projects/lvg/current/docs/userDoc/install/install.html\n",
    "\n",
    "# check that delimiter is not a word found as an entity\n",
    "delimiter = 'delim'\n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    if (entities['word'] == delimiter).any():\n",
    "        print('Match found.') # we don't want to see this\n",
    "\n",
    "# write all entities to a text file\n",
    "# delimiter is used because the UMLS Norm tool sometimes outputs more than one line of output per input word\n",
    "# so this is a crude way of marking which lines in the output correspond to which line in the input\n",
    "with open('NormInput.txt', 'w', encoding='utf-8') as f:\n",
    "    for article, [entities, origin] in raw_combined_data.items():\n",
    "        f.write(f'\\n{delimiter}\\n'.join(entities['word'].tolist()))\n",
    "        f.write(f'\\n{delimiter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87121484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the norm tool should have been run externally on the files\n",
    "# for more information about Norm tool see here: https://lhncbc.nlm.nih.gov/LSG/Projects/lvg/current/docs/userDoc/install/install.html\n",
    "\n",
    "# read in the Norm tool output file, process each line of output\n",
    "normalized_words = []\n",
    "with open('NormOutput.txt', 'r', encoding='utf-8') as f:\n",
    "    cache = ''\n",
    "    while True:\n",
    "        line = f.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        if line == f'{delimiter}|{delimiter}':\n",
    "            normalized_words.append(cache)\n",
    "        else:\n",
    "            cache = line\n",
    "normalized_words = [word.split('|')[1] for word in normalized_words]\n",
    "\n",
    "# populate the \"normalized word\" column of the corresponding entities dataframe \n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    n = len(entities['word'])\n",
    "    normalized_words_subset = normalized_words[:n]\n",
    "    del normalized_words[:n]\n",
    "    entities['normalized word'] = normalized_words_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c4682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## VISUALIZE DISTRIBUTION OF WORD LENGTHS ##########\n",
    "\n",
    "# calculate the frequency of each word\n",
    "disease_words = np.array([])\n",
    "unnormed_disease_words = np.array([])\n",
    "# word_lengths = np.array([])\n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    # aggregate every disease words instance\n",
    "    disease_words = np.append(disease_words, entities[entities['entity'] == 'disease']['normalized word'].astype('string').to_numpy())\n",
    "    unnormed_disease_words = np.append(unnormed_disease_words, entities[entities['entity'] == 'disease']['word'].astype('string').to_numpy())\n",
    "\n",
    "# calculate frequency of each distinct word\n",
    "uniq_disease_words, disease_counts = np.unique(disease_words, return_counts=True)\n",
    "uniq_unnormed_disease_words = np.unique(unnormed_disease_words)\n",
    "\n",
    "# sort by frequency\n",
    "disease_by_counts = {word: count for count, word in sorted(zip(disease_counts, uniq_disease_words), reverse=True)}\n",
    "disease_by_counts = pd.DataFrame({\n",
    "    'word': disease_by_counts.keys(),\n",
    "    'count': disease_by_counts.values()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c410d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CATEGORIZATION OF DISEASE ##########\n",
    "\n",
    "# performed manual categorization for the first 100 entries in most_frequent_disease.xlsx sheet separately prior to the following\n",
    "\n",
    "# sort the general categories of diseases\n",
    "disease_categories = pd.read_excel('most_frequent_diseases.xlsx')\n",
    "categories_counts = {}\n",
    "disease_categories.fillna('', inplace=True)\n",
    "for index, row in disease_categories.iterrows():\n",
    "    if row['categorization'] == '':\n",
    "        continue\n",
    "    curr = categories_counts.get(row['categorization'], 0)\n",
    "    categories_counts[row['categorization']] = curr + int(row['count'])\n",
    "categories_by_counts = dict(sorted(categories_counts.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac4ad1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cancer': 502,\n",
       " 'breast cancer': 188,\n",
       " 'cardiovascular disease': 134,\n",
       " 'neurological disease': 115,\n",
       " 'blood cancer': 87,\n",
       " 'lung cancer': 56,\n",
       " 'prostate cancer': 56,\n",
       " 'brain cancer': 54,\n",
       " 'liver cancer': 49,\n",
       " 'mental health disorder': 42,\n",
       " 'stomach cancer': 37,\n",
       " 'colorectal cancer': 28,\n",
       " 'viral infection': 20,\n",
       " 'obesity': 15,\n",
       " 'pancreatic cancer': 13,\n",
       " 'skin cancer': 12,\n",
       " 'lymphatic cancer': 11,\n",
       " 'diabetes': 11,\n",
       " 'immunodeficiency syndrome': 11,\n",
       " 'ovarian cancer': 10,\n",
       " 'kidney cancer': 10,\n",
       " 'esophageal cancer': 7,\n",
       " 'adenocarcinoma': 7,\n",
       " 'neuroblastoma': 6,\n",
       " 'pulmonary disease': 6,\n",
       " 'arthritis': 6,\n",
       " 'nasopharyngeal cancer': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_by_counts\n",
    "# top 5 general categories:\n",
    "# 1) breast cancer\n",
    "# 2) cardiovascular disease\n",
    "# 3) neurological disease\n",
    "# 4) blood cancer\n",
    "# 5) lung cancer\n",
    "# \"cancer\" is by far the most abundant entity, but it is not \n",
    "# specific enough to be its own category. However, this suggests \n",
    "# that other types of cancer may deserve more weight than \n",
    "# cardiovascular or neurological disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd22410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find articles enriched for the words corresponding to the\n",
    "# top 5 diseases of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e0aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find keywords for each of the selected diseases\n",
    "keywords_by_disease = {\n",
    "    'breast cancer': [],\n",
    "    'cardiovascular disease': [],\n",
    "    'neurological disease': [],\n",
    "    'blood cancer': [],\n",
    "    'lung cancer': []\n",
    "}\n",
    "for disease in keywords_by_disease.keys():\n",
    "    keywords_by_disease[disease] = list(disease_categories.loc[disease_categories['categorization'] == disease]['word'])\n",
    "\n",
    "# mark appearance of keywords for each article\n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    for disease, keywords in keywords_by_disease.items():\n",
    "        entities[f'is_{disease}_keyword'] = entities.apply(lambda row : 1 if row['normalized word'] in keywords else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81784c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_articles = 5 # number of articles per disease category\n",
    "articles_by_disease = {\n",
    "    'breast cancer': [],\n",
    "    'cardiovascular disease': [],\n",
    "    'neurological disease': [],\n",
    "    'blood cancer': [],\n",
    "    'lung cancer': []\n",
    "}\n",
    "\n",
    "# select n_articles number of articles most enriched for keywords for each disease category\n",
    "for disease in keywords_by_disease.keys():\n",
    "    articles_by_keyword_freqs = {}\n",
    "    for article, [entities, origin] in raw_combined_data.items():\n",
    "        articles_by_keyword_freqs[article] = sum(entities[f'is_{disease}_keyword'])\n",
    "    articles_by_disease[disease] = list(sorted(articles_by_keyword_freqs.items(), key=lambda i: i[1], reverse=True)[:n_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19bfcc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'breast cancer': [('10.1016/j.jcpa.2012.01.021', 13),\n",
       "  ('10.1016/j.bbagrm.2019.03.002', 12),\n",
       "  ('10.1016/j.bbrc.2019.02.088', 12),\n",
       "  ('10.1016/j.gene.2022.146463', 10),\n",
       "  ('10.1016/j.freeradbiomed.2016.08.031', 9)],\n",
       " 'cardiovascular disease': [('10.1016/j.tcm.2015.08.006', 18),\n",
       "  ('10.1007/s10741-015-9483-x', 10),\n",
       "  ('10.1038/s41371-019-0218-7', 9),\n",
       "  ('10.1016/j.bbadis.2020.165836', 8),\n",
       "  ('10.1016/j.neuint.2019.03.004', 7)],\n",
       " 'neurological disease': [('10.1007/s00401-017-1732-8', 13),\n",
       "  ('10.1016/j.nbd.2014.11.023', 9),\n",
       "  ('10.1016/j.biopha.2018.01.110', 8),\n",
       "  ('10.1007/s10571-013-0012-y', 8),\n",
       "  ('10.1007/s11060-018-03018-6', 8)],\n",
       " 'blood cancer': [('10.1038/leu.2010.276', 16),\n",
       "  ('10.1038/leu.2012.86', 15),\n",
       "  ('10.1016/j.leukres.2005.05.010', 8),\n",
       "  ('10.1016/j.mehy.2013.04.021', 8),\n",
       "  ('10.1007/s00018-018-2895-8', 8)],\n",
       " 'lung cancer': [('10.1053/j.seminoncol.2005.07.007', 9),\n",
       "  ('10.1007/s10555-015-9563-3', 9),\n",
       "  ('10.1016/j.jss.2003.11.024', 6),\n",
       "  ('10.3816/CLC.2008.n.053', 5),\n",
       "  ('10.1038/sj.onc.1209068', 5)]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_by_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7ca50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atc(doi):\n",
    "    \"\"\"Fetches article text data given doi.\n",
    "\n",
    "    Args:\n",
    "        doi: the article doi\n",
    "\n",
    "    Returns:\n",
    "        The text of the article as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if doi in cell_data:\n",
    "        return cell_data[doi][0]\n",
    "    elif doi in nature_data:\n",
    "        return nature_data[doi][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "breast_cancer_atcs = [get_atc(doi) for doi, __ in articles_by_disease['breast cancer']]\n",
    "cardiovascular_disease_atcs = [get_atc(doi) for doi, __ in articles_by_disease['cardiovascular disease']]\n",
    "neurological_disease_atcs = [get_atc(doi) for doi, __ in articles_by_disease['neurological disease']]\n",
    "blood_cancer_atcs = [get_atc(doi) for doi, __ in articles_by_disease['blood cancer']]\n",
    "lung_cancer_atcs = [get_atc(doi) for doi, __ in articles_by_disease['lung cancer']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
