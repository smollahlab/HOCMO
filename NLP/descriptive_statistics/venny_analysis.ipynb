{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375108c4",
   "metadata": {},
   "source": [
    "# Code for Venny\n",
    "\n",
    "This notebook contains code that is used for calculating the overlap of terms between genetic entities that were extracted from the NER model against the terms that are found in the crewDB datatables (reader, eraser, writer csv tables). The overlap in terms was visualized using the online Venn Diagram generator Venny 2.1.0 (https://bioinfogp.cnb.csic.es/tools/venny/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a9276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c66e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '..' # directory that contains NER output pickle files and crewDB csv tables\n",
    "\n",
    "########## PREPROCESSING ##########\n",
    "\n",
    "# load data from pickle files\n",
    "# discard the \"text\" data and mark which journal each article originated from\n",
    "with open(f'{dir}/savedCellArticles.pkl', 'rb') as f:\n",
    "    cell_data = pickle.load(f) # 924 articles\n",
    "    filtered_cell_data = {article: [entities, 'cell'] for article, [text, entities] in cell_data.items()}\n",
    "\n",
    "with open(f'{dir}/savedNatureArticles.pkl', 'rb') as f:\n",
    "    nature_data = pickle.load(f) # 401 articles\n",
    "    filtered_nature_data = {article: [entities, 'nature'] for article, [text, entities] in nature_data.items()}\n",
    "\n",
    "# raw combined data of the form {article: [entities, origin]}\n",
    "raw_combined_data = filtered_cell_data | filtered_nature_data # 1325 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54608f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UMLS LEXICON NORMALIZATION ##########\n",
    "# for more information about Norm tool see here: https://lhncbc.nlm.nih.gov/LSG/Projects/lvg/current/docs/userDoc/install/install.html\n",
    "\n",
    "# read in the Norm tool output file, process each line of output\n",
    "normalized_words = []\n",
    "delimiter = 'delim'\n",
    "with open(f'NormOutput.txt', 'r', encoding='utf-8') as f:\n",
    "    cache = ''\n",
    "    while True:\n",
    "        line = f.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        if line == f'{delimiter}|{delimiter}':\n",
    "            normalized_words.append(cache)\n",
    "        else:\n",
    "            cache = line\n",
    "normalized_words = [word.split('|')[1] for word in normalized_words]\n",
    "\n",
    "# populate the \"normalized word\" column of the corresponding entities dataframe \n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    n = len(entities['word'])\n",
    "    normalized_words_subset = normalized_words[:n]\n",
    "    del normalized_words[:n]\n",
    "    entities['normalized word'] = normalized_words_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034f2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## FILTER LONG WORDS ##########\n",
    "\n",
    "# remove words which are longer than 150 characters\n",
    "filtered_combined_data = {}\n",
    "for article, [entities, origin] in raw_combined_data.items():\n",
    "    filtered_combined_data[article] = [entities[entities['word'].str.len() < 125], origin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## UNIQUE TERM FINDING ##########\n",
    "\n",
    "# make unique list of all words that were extracted by the NER\n",
    "unique_words = []\n",
    "for doi, [entities, origin] in filtered_combined_data.items():\n",
    "    unique_words += list(entities[entities['entity'] == 'genetic']['word'])\n",
    "unique_words = list(set(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878a48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all terms in the crewDB database\n",
    "cr_types = ['reader', 'eraser', 'writer']\n",
    "unique_crs = []\n",
    "for cr_type in cr_types:\n",
    "    crew_df = pd.read_csv(f'{dir}/{cr_type}tbl.csv')\n",
    "    unique_crs += list(crew_df['gene'])\n",
    "    unique_crs += list(crew_df['domain'])\n",
    "    for target_entity in list(crew_df['Target entity'].dropna()):\n",
    "        unique_crs += [entity.strip() for entity in target_entity.split(',')]\n",
    "unique_crs = list(set(unique_crs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398d6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "venny_dir = 'venny_files' # directory to store venny files\n",
    "\n",
    "# write unique terms to txt file to be inputted to venny\n",
    "with open(f'{venny_dir}/unique_NER.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(unique_words))\n",
    "with open(f'{venny_dir}/unique_crewDB.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(unique_crs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
